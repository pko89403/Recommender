{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMF_with_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1zgRhzmjMasGPiB89kdChA_AW6BJeOsUk",
      "authorship_tag": "ABX9TyP5B3bcnUGTamI5IvqOZmgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pko89403/RecsysStudy/blob/master/NeuralCollaborativeFiltering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k20bCc9MdL09"
      },
      "source": [
        "# GMF 를 Tensorflow로 구현 \n",
        "\n",
        "## 사용하는 데이터 \n",
        "- lastfm 데이터셋 \n",
        "## Evaluation\n",
        "- top@K ( K = 10 )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvN3-btgeQxF",
        "outputId": "dd9f3658-c7a3-427e-9cbc-f43dd729d18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls \"/content/drive/My Drive/data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb  imdb.tar.gz  index.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-cX-Jl1c_WR",
        "outputId": "bf74a4d7-b1cf-4c2f-ef8f-7570adb9d228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!wget \"http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz\" -P \"/content/drive/My Drive/data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-26 05:57:23--  http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz\n",
            "Resolving mtg.upf.edu (mtg.upf.edu)... 84.89.139.55\n",
            "Connecting to mtg.upf.edu (mtg.upf.edu)|84.89.139.55|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 569202935 (543M) [application/x-gzip]\n",
            "Saving to: ‘/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz’\n",
            "\n",
            "lastfm-dataset-360K 100%[===================>] 542.83M  1.25MB/s    in 8m 27s  \n",
            "\n",
            "2020-10-26 06:05:51 (1.07 MB/s) - ‘/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz’ saved [569202935/569202935]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHlTSllZeTuz",
        "outputId": "fc6234fa-8583-485b-d88f-3f95ed6596da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!tar -zxvf \"/content/drive/My Drive/data/lastfm-dataset-360K.tar.gz\" -C \"/content/drive/My Drive/data/lastfm/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lastfm-dataset-360K/\n",
            "lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\n",
            "lastfm-dataset-360K/README.txt\n",
            "lastfm-dataset-360K/mbox_sha1sum.py\n",
            "lastfm-dataset-360K/usersha1-profile.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NfgxyWnjHCx",
        "outputId": "5d01759f-5b25-4b7f-8f9f-000ef3bdc082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!head \"/content/drive/My Drive/data/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t3bd73256-3905-4f3a-97e2-8b341527f805\tbetty blowtorch\t2137\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tf2fb0ff0-5679-42ec-a55c-15109ce6e320\tdie Ärzte\t1099\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tb3ae82c2-e60b-4551-a76d-6620f1b456aa\tmelissa etheridge\t897\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t3d6bbeb7-f90e-4d10-b440-e153c0d10b53\telvenking\t717\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tbbd2ffd7-17f4-4506-8572-c1ea58c3f9a8\tjuliette & the licks\t706\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t8bfac288-ccc5-448d-9573-c33ea2aa5c30\tred hot chili peppers\t691\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t6531c8b1-76ea-4141-b270-eb1ac5b41375\tmagica\t545\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t21f3573f-10cf-44b3-aeaa-26cccd8448b5\tthe black dahlia murder\t507\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\tc5db90c4-580d-4f33-b364-fbaa5a3a58b5\tthe murmurs\t424\n",
            "00000c289a1829a808ac09c00daf10bc3c4e223b\t0639533a-0402-40ba-b6e0-18b067198b73\tlunachicks\t403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeI6qAidfTrR",
        "outputId": "97b9c152-3568-47a4-f66a-b77afe39551f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00000c289a1829a808ac09c00daf10bc3c4e223b</th>\n",
              "      <th>3bd73256-3905-4f3a-97e2-8b341527f805</th>\n",
              "      <th>betty blowtorch</th>\n",
              "      <th>2137</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>f2fb0ff0-5679-42ec-a55c-15109ce6e320</td>\n",
              "      <td>die Ärzte</td>\n",
              "      <td>1099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>b3ae82c2-e60b-4551-a76d-6620f1b456aa</td>\n",
              "      <td>melissa etheridge</td>\n",
              "      <td>897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>3d6bbeb7-f90e-4d10-b440-e153c0d10b53</td>\n",
              "      <td>elvenking</td>\n",
              "      <td>717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8</td>\n",
              "      <td>juliette &amp; the licks</td>\n",
              "      <td>706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>8bfac288-ccc5-448d-9573-c33ea2aa5c30</td>\n",
              "      <td>red hot chili peppers</td>\n",
              "      <td>691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   00000c289a1829a808ac09c00daf10bc3c4e223b  ...  2137\n",
              "0  00000c289a1829a808ac09c00daf10bc3c4e223b  ...  1099\n",
              "1  00000c289a1829a808ac09c00daf10bc3c4e223b  ...   897\n",
              "2  00000c289a1829a808ac09c00daf10bc3c4e223b  ...   717\n",
              "3  00000c289a1829a808ac09c00daf10bc3c4e223b  ...   706\n",
              "4  00000c289a1829a808ac09c00daf10bc3c4e223b  ...   691\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5TW00QS0kAd"
      },
      "source": [
        "def load_dataset():\n",
        "  \n",
        "  \n",
        "  df = pd.read_csv(\"/content/drive/My Drive/data/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\", sep='\\t')\n",
        "  \n",
        "\n",
        "  df = df.drop(df.columns[1], axis=1)\n",
        "  df.columns = ['user', 'item', 'plays']\n",
        "  df = df.dropna()\n",
        "  df = df.loc[df.plays != 0]\n",
        "\n",
        "\n",
        "  df_count = df.groupby(['user']).count()\n",
        "  df['count'] = df.groupby('user')['user'].transform('count')\n",
        "  df = df[df['count'] > 1]\n",
        "\n",
        "  \n",
        "  # Return Series of codes as well as the index.\n",
        "  df['user_id'] = df['user'].astype('category').cat.codes\n",
        "  df['item_id'] = df['item'].astype('category').cat.codes  \n",
        "\n",
        "\n",
        "  item_lookup = df[['item_id', 'item']].drop_duplicates()\n",
        "  item_lookup['item_id'] = item_lookup.item_id.astype(str)\n",
        "\n",
        "\n",
        "  user_lookup = df[['user_id', 'user']].drop_duplicates()\n",
        "  user_lookup['user_id'] = user_lookup.user_id.astype(str)\n",
        "\n",
        "\n",
        "  df = df[['user_id', 'item_id', 'plays']]\n",
        "  users = list(np.sort(df.user_id.unique()))\n",
        "  items = list(np.sort(df.item_id.unique()))\n",
        "\n",
        "\n",
        "  df_train, df_test = train_test_split(df)\n",
        "\n",
        "\n",
        "  rows = df_train.user_id.astype(int)\n",
        "  cols = df_train.item_id.astype(int)\n",
        "\n",
        "  values = list(df_train.plays)\n",
        "\n",
        "  uids = np.array(rows.tolist())\n",
        "  iids = np.array(cols.tolist())\n",
        "\n",
        "\n",
        "  df_neg = get_negatives(uids, iids, items, df_test)\n",
        "\n",
        "  return uids, iids, df_train, df_test, df_neg, users, items, item_lookup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IW589VqmvB8"
      },
      "source": [
        "def mask_first(x):\n",
        "  result = np.ones_like(x)\n",
        "  result[0] = 0\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5NGfR1qd1xl"
      },
      "source": [
        "def train_test_split(df):\n",
        "  df_test = df.copy(deep=True)\n",
        "  df_train = df.copy(deep=True)\n",
        "\n",
        "  # Group by user and select only the first item for each user \n",
        "  df_test = df_test.groupby(['user_id']).first()\n",
        "  df_test['user_id'] = df_test.index\n",
        "  df_test = df_test[['user_id', 'item_id', 'plays']]\n",
        "  print(df_test.index.name)\n",
        "  df_test.rename(index={'name': ''}, inplace=True)\n",
        "\n",
        "  # Remove the same items as we for our test set in our training set\n",
        "  mask = df.groupby(['user_id'])['user_id'].transform(mask_first).astype(bool)\n",
        "  df_train = df.loc[mask]\n",
        "\n",
        "  return df_train, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLvChBZ3n67b"
      },
      "source": [
        "def get_negatives(uids, iids, items, df_test):\n",
        "  negativeList = []\n",
        "  test_u = df_test['user_id'].values.tolist()\n",
        "  test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "  test_ratings = list(zip(test_u, test_i))\n",
        "  zipped = set(zip(uids, iids))\n",
        "\n",
        "  for (u, i) in test_ratings:\n",
        "    negatives = []\n",
        "    negatives.append((u, i))\n",
        "    for t in range(100):\n",
        "      j = np.random.randint(len(items)) # Get random item id\n",
        "      while (u, j) in zipped: # Check if there is an interaction\n",
        "        j = np.random.randint(len(items)) # If yes, generate a new item id \n",
        "      negatives.append(j) # Once a negative interaction is found we add it\n",
        "    negativeList.append(negatives)\n",
        "\n",
        "  df_neg = pd.DataFrame(negativeList)\n",
        "\n",
        "  return df_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c386jd_qqTd1"
      },
      "source": [
        "def get_train_instances():\n",
        "  user_input, item_input, labels = [], [], []\n",
        "  zipped = set(zip(uids, iids))\n",
        "\n",
        "  for (u, i) in zip(uids, iids):\n",
        "    # Add positive interaction\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "\n",
        "    # Sample random negative interaction\n",
        "    for t in range(num_neg):\n",
        "      j = np.random.randint(len(items))\n",
        "      while (u, i) in zipped:\n",
        "        j = np.random.randint(len(items))\n",
        "      \n",
        "      user_input.append(u)\n",
        "      item_input.append(j)\n",
        "      labels.append(0)\n",
        "\n",
        "    return user_input, item_input, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWdg5RRysvRd"
      },
      "source": [
        "def random_mini_batches(U, I, L, mini_batch_size=256):\n",
        "  mini_batches = []\n",
        "  \n",
        "  shuffled_U, shuffled_I, shuffled_L = shuffle(U, I, L)\n",
        "\n",
        "  num_complete_batches = int(math.floor(len(U)/mini_batch_size))\n",
        "  for k in range(0, num_complete_batches):\n",
        "    mini_batch_U = shuffled_U[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "    mini_batch_I = shuffled_I[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "    mini_batch_L = shuffled_L[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "\n",
        "    mini_batch = (mini_batch_U, mini_batch_I, mini_batch_L)\n",
        "    mini_batches.append(mini_batch)\n",
        "  \n",
        "  if len(U) % mini_batch_size != 0:\n",
        "    mini_batch_U = shuffled_U[num_complete_batches * mini_batch_size : len(U)]\n",
        "    mini_batch_I = shuffled_I[num_complete_batches * mini_batch_size : len(U)]\n",
        "    mini_batch_L = shuffled_L[num_complete_batches * mini_batch_size : len(U)]\n",
        "\n",
        "    mini_batch = (mini_batch_U, mini_batch_I, mini_batch_L)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "  return mini_batches "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Dwmd1svTJf"
      },
      "source": [
        "def get_hits(k_ranked, holdout):\n",
        "  for item in h_ranked:\n",
        "    if item == holdout:\n",
        "      return 1\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NA44Fe7vgb_"
      },
      "source": [
        "def eval_rating(idx, test_ratings, test_negatives, K):\n",
        "  map_item_score = {}\n",
        "\n",
        "  # get the negative interactions our userr\n",
        "  items = test_negatives[idx]\n",
        "  # Get the user idx\n",
        "  user_idx = test_ratings[idx][0]\n",
        "  # Get the item idx -> holdout item\n",
        "  holdout = test_ratings[idx][1]\n",
        "\n",
        "  # Add the holdout to the end of the negative interactions list.\n",
        "  items.append(holdout)\n",
        "\n",
        "  # Prepare our user and item arrays for tensorflow\n",
        "  predict_user = np.full(len(items), user_idx, dtype='int32').reshape(-1,1)\n",
        "  np_items = np.array(items).reshape(-1, 1)\n",
        "\n",
        "  # Feed user and items into the TF graph.\n",
        "  predictions = session.run([output_layer], feed_dict={user: predict_user, item: np_items})\n",
        "\n",
        "  # Get the predicted score to item id \n",
        "  for i in range(len(items)):\n",
        "    current_item = items[i]\n",
        "    map_item_score[current_item] = predictions[i]\n",
        "  \n",
        "  # Get the K highest ranked items as a list\n",
        "  h_ranked = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
        "\n",
        "  # Get a list of hit or no hit.\n",
        "  hits = get_hits(h_ranked, holdout)\n",
        "\n",
        "  return hits \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NjRbAW2ykQU"
      },
      "source": [
        "def evaluate(df_neg, K=10):\n",
        "  hits = []\n",
        "\n",
        "  test_u = df_test['user_id'].values.tolist()\n",
        "  test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "  test_ratings = list(zip(test_u, test_i))\n",
        "\n",
        "  df_neg = df_neg.drop(df_neg.columns[0], axis=1)\n",
        "  test_negatives = df_neg.values.tolist()\n",
        "\n",
        "  for idx in range(len(test_ratings)):\n",
        "    hitrate = eval_rating(idx, test_ratings, test_negatives, K)\n",
        "    hits.append(hitrate)\n",
        "\n",
        "  return hits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOSF45Ez9cU"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import math\n",
        "import heapq\n",
        "from tqdm import tqdm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZKNn4v0T9i",
        "outputId": "6ddc668c-30c3-491c-8fa0-8b19117a7153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load and prepare our data\n",
        "uids, iids, df_train, df_test, df_neg, users, items, item_lookup = load_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO8NRWxMcbuu"
      },
      "source": [
        "# Hyper-Parameters\n",
        "num_neg = 4\n",
        "epoch = 10\n",
        "batch_size = 256\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFDFbDkTcn0r",
        "outputId": "1ba1a357-d8ba-4ec5-902e-09ba0817d83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Set up Tensorflow Graph\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  # Define input placeholders for user, item and label\n",
        "  user = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "  item = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "  label = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "\n",
        "  # User Feature Embedding\n",
        "  u_var = tf.Variable(tf.random_normal([len(users), 32], stddev=0.05), name='user_embedding')\n",
        "  user_embedding = tf.nn.embedding_lookup(u_var, user)\n",
        "\n",
        "  # Item Feature Embedding\n",
        "  i_var = tf.Variable(tf.random_normal([len(items), 32], stddev=0.05), name='item_embedding')\n",
        "  item_embedding = tf.nn.embedding_lookup(i_var, item)\n",
        "\n",
        "  # Flatten Our User and Item Embedding\n",
        "  user_embedding = tf.keras.layers.Flatten()(user_embedding)\n",
        "  item_embedding = tf.keras.layers.Flatten()(item_embedding)\n",
        "\n",
        "  # Concatenate our two embedding vectors together\n",
        "  concatenated = tf.keras.layers.concatenate([user_embedding, item_embedding])\n",
        "\n",
        "  # Add a first dropout layer\n",
        "  dropout = tf.keras.layers.Dropout(0.2)(concatenated)\n",
        "\n",
        "  # Below we add our four hidden layers along with batch\n",
        "  # Normalization and Dropout. We use relu as the Activation Funtion\n",
        "  layer_1 = tf.keras.layers.Dense(64, activation='relu', name='layer1')(dropout)\n",
        "  batch_norm1 = tf.keras.layers.BatchNormalization(name='batch_norm1')(layer_1)\n",
        "  dropout1 = tf.keras.layers.Dropout(0.2, name='dropout1')(batch_norm1)\n",
        "\n",
        "  layer_2 = tf.keras.layers.Dense(32, activation='relu', name='layer2')(dropout1)\n",
        "  batch_norm2 = tf.keras.layers.BatchNormalization(name='batch_norm2')(layer_2)\n",
        "  dropout2 = tf.keras.layers.Dropout(0.2, name='dropout2')(batch_norm2)\n",
        "\n",
        "  layer_3 = tf.keras.layers.Dense(16, activation='relu', name='layer3')(dropout2)\n",
        "  layer_4 = tf.keras.layers.Dense(8, activation='relu', name='layer4')(layer_3)\n",
        "\n",
        "  # Our final single neuron output layer\n",
        "  output_layer = tf.keras.layers.Dense(1,\n",
        "    kernel_initializer=\"lecun_uniform\",\n",
        "    name='output_layer')(layer_4)\n",
        "\n",
        "  # Define Loss Function as Cross Entropy\n",
        "  labels = tf.cast(label, tf.float32)\n",
        "  logits = output_layer\n",
        "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "    labels=labels,\n",
        "    logits=logits))\n",
        "  \n",
        "  # Train using the Adam optimizer to minimize our loss \n",
        "  opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  step = opt.minimize(loss)\n",
        "\n",
        "  # Initialize all tensorflow variables\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "session = tf.session(config=None, graph=graph)\n",
        "session.run(init)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3f409a4beec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up Tensorflow Graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Define input placeholders for user, item and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzoXhmLOjZf1"
      },
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "  # Get our training input.\n",
        "  user_input, item_input, labels = get_train_instances()\n",
        "\n",
        "  # Generate a list of minibatches\n",
        "  minibatches = random_mini_batches(user_input, item_input, labels)\n",
        "\n",
        "  # This has nothing to do with tensorflow but gives\n",
        "  # us a nice progress bar for the training \n",
        "  progress = tqdm(total=len(minibatches))\n",
        "\n",
        "  # Loop over each batch and feed our users, items and labels\n",
        "  # into our graph.\n",
        "  for minibatch in minibatches:\n",
        "    feed_dict = {user: np.array(minibatch[0]).reshape(-1,1),\n",
        "                 item: np.array(minibatch[1]).reshape(-1,1),\n",
        "                 label: np.array(minibatch[2]).reshape(-1,1)}\n",
        "\n",
        "    _, l = session.run([step, loss], feed_dict)\n",
        "\n",
        "    # Update the progress \n",
        "    progress.update(1)\n",
        "    progress.set_description('Epoch: %d - Loss: %.3f' % (epoch+1, l))\n",
        "  \n",
        "  progress.close()\n",
        "\n",
        "  # Calculate top@K\n",
        "  hits = evaluate(df_neg)\n",
        "  print(np.array(hits).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeM7GjGNjsX1"
      },
      "source": [
        ""
      ]
    }
  ]
}